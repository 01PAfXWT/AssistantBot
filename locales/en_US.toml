model = "Model"
bot = "Bot"
chat_bot = "Chat bot"
completion_bot = "Completion bot"
baidu = "Baidu"
openai = "OpenAI"
openai_compatible = "OpenAI Compatible"
qwen = "Qwen"
ollama = "Ollama"

add = "Add"
update = "Update"
delete = "Delete"
save = "Save"
cancel = "Cancel"
confirm = "Confirm"
generate = "Generate"
query = "Query"
querying = "Querying"
backup = "Backup"
restore = "Restore"
note = "Note"
test = "Test"

chat = "Chat"
completion = "Completion"
chats = "Chats"
completions = "Completions"
prompt = "Prompt"
content = "Content"

paid = "Paid"
free = "Free"

proxy = "Proxy"

enable = "Enable"

load_archive = "Load Archive"
prompt_management = "Prompt Management"
settings = "Settings"

reload_app = "Reload App"
open_archive_folder = "Open DATA"
open_backup_folder = "Open Backup"

no_available_models_error = "No available models found. Navigate to `Settings` to add a model, or enable a LLM provider, or run Ollama and then refresh the page."

ollama_model_disabled = "The Ollama model is disabled. Navigate `Settings` to enable Ollama."
ollama_server_not_running = "Ollama server is not running. Please check if Ollama is running."
ollama_model_updated = "Ollama model list updated."
query_ollama_model_failed = "Failed to query Ollama model. Please check if Ollama is running. Error occurred: {e}."

select_a_function = "Select a Function"
configuration = "Configuration"
model_alias = "Model Alias"
conversation_round = "Conversation Rounds"
conversation_round_notice = "- Conversation Rounds: The number of historical chat messages retained for the next round of conversation. The Default is 6.\n - The rounds of conversation should be set based on the context length of the model used.\n - The final chat message input by the user counts as one conversation round."
backup_llms_configs = "Backup LLM Configurations"
backup_llms_configs_success = "Backup Successful"

load_archive_label = "Load a `{type}` archive to continue `{work}`:"

archive_management_caption = "You can view and manage the content of `{chat}` or `{completion}` archives in the project archive folder, rename or delete archives."
no_archive_found = "No archives found in the `{archive_folder}` folder."

unable_load_configs = "Unable to load config.toml configuration file."
settings_caption = "Configure general settings and LLM service provider settings."
common_settings = "Common Settings"
language = "Language"
select_a_language = "Select a Language"
language_switched_info = "Language switched from `{old_language}` to `{new_language}`. The app will reload shortly to switch the language."

web_port = "Web Port"
web_port_notice = "Whether to use a random port. By default, the application starts with a random port to avoid issues with the port being occupied, which could prevent the application from starting or reloading. If deploying remotely, it is recommended to use a fixed port to facilitate firewall rule settings. The default port is 8501, which is Streamlitâ€™s default port, but you can modify it as needed."
random_web_port = "Random Web Port"
random_web_port_notice = "Web port setting changed. The app will reload shortly using the assigned web port. If the app cannot successfully reload after this message is shown, it means the assigned web port is currently occupied. Please reassign a web port."
set_web_port = "Set Web Port, default is 8501"
web_port_set_to = "Web port set to: {web_port}."

data_folder_location = "DATA Folder Location"
data_folder_description = "The default location for the DATA folder is the `{default_data_folder}` folder in the project root directory. It can be set to another folder as needed, such as a cloud storage folder like OneDrive to sync data."
new_location = "New Location"
data_folder_set_to = "Set to: `{new_archive_folder}`"
data_folder_notice = "The existing files and subdirectories in `{current_location}` will be moved to the new location `{new_archive_folder}`."
data_folder_clear_notice = "The existing files and subdirectories in `{new_archive_folder}` will be cleared."
data_folder_error = "`{new_archive_folder}` is not a valid folder path."
data_folder_updated = "Data folder location updated."

refresh_interval = "Page refresh interval"
refresh_interval_notice = "Set the page refresh interval in seconds."
refresh_interval_set_to = "Refresh interval set to: {refresh_interval} seconds."
refresh_interval_updated = "Refresh interval updated."

proxy_setting_notice = "Set up a proxy server. Default is local proxy, http: `http://127.0.0.1:10809`, https: `http://127.0.0.1:10809`."
http_proxy = "HTTP Proxy"
https_proxy = "HTTPS Proxy"
proxy_test_description = "Test the HTTP and HTTPS proxy settings by connecting to Google.com."
http_proxy_success = "HTTP Proxy ({proxy}) test passed."
https_proxy_success = "HTTPS Proxy ({proxy}) test passed."
http_proxy_failed = "Http proxy ({proxy}) test failed."
https_proxy_failed = "Https proxy ({proxy}) test failed."
proxy_not_set = "Proxy not set."

verbose_mode = "Verbose Mode"
verbose_mode_description = "Enable verbose mode to display information about messages and parameter values in the console during communication with LLM. This is especially useful for troubleshooting."
verbose_mode_label = "Enable verbose mode"
verbose_mode_updated = "Verbose mode setting updated."

llm_settings = "LLM Settings"
activate_llm_provider = "LLM Provider"
llm_provider_activation_changed = "LLM provider activations updated."

conversation_round_updated = "Conversation rounds set to: {conversation_round}."

max_number_of_completions = "Max Number of Completions"
max_number_of_completions_description = "Set the maximum number of completions cached on the page, the default is 3."
max_number_of_completions_updated = "Max number of completions set to: {max_completions}."

generating_content_notice = "generating content according to `{prompt}`..."
generating_content = "generating content..."
show_generated_content_title = "`{number}` `{completion}` generated"
reach_max_completions_notice = "The maximum number of completions has been reached. The oldest record will be overwritten."
content_generated_notice = "@{time}: Content generated successfully. Copied to clipboard."

select_a_provider = "Select a LLM Provider to Configure"
invalid_content_type = "Invalid content type, should be chat or completion."

clear_cache_button = "Clear {content_type} Cache"
save_to_repository_button  = "Save to Repository"
download_history_button = "Download {content_type} History"
clear_cached_history = "Clear Cached History"
no_cached_history = "No cached history."
content_saved = "{content_type_} content saved."
save_to_repository = "Save `{file_name}` to: `{repository_path}`."
download_history = "Download cached {content_type_} history, default file name {file_name}."

proxy_status = "Using proxy to connect to {provider} model. Proxy settings: {proxies}."

prompt_management_caption = "Add, modify, or delete `{chat}` system prompts or `{completion}` prompts."
select_a_prompt_type = "Select a Prompt Type"
no_empty_prompt = "Prompt cannot be empty."
no_predefined_prompts = "No predefined prompts."
selected_prompt = "Selected Prompt"
new_prompt = "New Prompt"
add_to_repository_label = "Add to {prompt_type} Prompt Repository"
add_new_prompt_info = "New {prompt_type} prompt: {prompt} added, page will refresh automatically."

invalid_prompt_type = "Invalid prompt type."
system_prompt = "System Prompt"
select_system_prompt_label = "Select a predefined system prompt or role prompt."
select_prompt_label = "Select a Prompt"

select_an_archive_type = "Select an Archive Type"

archive_management = "Archive Management"
rename_archive = "Rename Archive"
delete_archive = "Delete Archive"

open_folder = "Open Archive Folder"
file_to_be_renamed = "File to be renamed: `{selected_archive_file}`."
rename = "Rename"
new_file_name = "New File Name"
file_to_be_deleted = "File to be deleted: `{selected_archive_file}`."

rename_archive_success = "`{old_name}` renamed to `{new_name}`, page will reload."
rename_error_file_exists = "Failed to rename `{old_name}` to `{new_name}`, file already exists."
delete_archive_success = "`{selected_archive_file}` deleted successfully, page will reload. If this was an accidental deletion, please manually recover it from the `recycle bin`."

selected_completion_archive = "A `{completion}` archive selected: `{file_name}`. It contains `{number}` {completions}."
show_prompt = "Show Prompt"
generated_completion_info = "@{time}: Generated Content of `{prompt}...` by `{model}`:"
copy_completion = "Copy Completion {index}"
delete_completion = "Delete Completion {index}"
content_copied = "Content copied to clipboard."
content_deleted = "Content deleted. The page will reload shortly."
selected_chat_archive = "A `{chat}` archive selected: `{file_name}`. It contains `{number}` {chats}."

# LLM Provider Settings Block
provider_setting_block = "{provider} Model Settings"
add_new_model_description = "Add New `{provider}` Model"
proxy_setting = "Proxy Setting"
proxy_setting_label = "Use Proxy to connect to `{provider}` model. Set according to your network environment, in conjunction with the Proxy setting in `Common Settings`."
# proxy_true = "Use Proxy"
# proxy_false = "Do Not Use Proxy"
proxy_setting_updated = "Proxy settings updated."
provider_models = "{provider} Models"
last_query_time = "Last query time: {time}"
provider_supported_models = "`{provider}` supported models:"
query_provider_models = "Click `Query` to check or update `{provider}` supported models."

provider_api_key_title = "{provider} API Key"
provider_api_key_description = "API Key is the authentication information for the `{provider}` platform."
provider_api_key_label = "{provider} API Key"
no_provider_api_key_found = "No `{provider}` API Key found, please add the API Key."
api_key_cannot_be_empty = "API Key cannot be empty."
api_key_updated = "{provider} API Key updated."

provider_model_not_found = "No added `{provider}` models found."
provider_model_management = "Manage Saved `{provider}` Models"
provider_select_a_model = "Select a model to query or modify its information."

baidu_auth_info = "Client ID and Client Secret are authentication credentials for accessing Baidu models. The Client ID is also known as AK, and the Client Secret is also known as SK."
baidu_client_id = "Baidu Client ID, aka, AK"
baidu_client_secret = "Baidu Client Secret, aka, SK"
baidu_auth_info_updated = "Baidu authentication information updated."

qianfan_supported_models = "Baidu qianfan SDK(`{version}`) Supported Models"
qianfan_update_notice = "Note: The list of models supported by the Qianfan SDK may be updated. It is recommended to use `pip install qianfan --upgrade` to update Qianfan and support the latest model list."
paid_model_list = "Paid Model List"
free_model_list = "Free Model List"
support_pass_system_prompt_by_parameter = "Models that accept the system prompt as a parameter"
not_support_pass_system_prompt_by_parameter = "Models that do not accept the system prompt as a parameter will automatically add the system prompt to the beginning of the first message"
baidu_model_name_description = "Baidu preset model names can be checked at https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Jlugqd6pw, or refer to the above `Qianfan SDK (v.{qianfan.version.VERSION}) supported model list`."
qwen_model_name_description = "Qwen preset model names can be checked at https://bailian.console.aliyun.com/#/model-market."
# model management block
model_management_description = "- `Model Alias` is a custom name for the model for easy identification, such as `OpenAI-GPT4-o`, `Baidu-ERNIE-4.0-Turbo`, `Qwen-MAX`.\n - `Model Name` is the unique identifier of the model on the `{provider}` platform, and must be the preset model name on the platform, such as `gpt4-o`, `ERNIE-4.0-Turbo-8K`, `qwen-max`, etc."
model_alias_label = "Model Alias"
model_name_label = "Model Name"
model_description_label = "Model Description"
model_description_with_colon = "Model Description:"
no_model_description = "No model description found."
model_architecture_description_label = "Model architecture: "
chat_support = "Chat Support"
completion_support = "Completion Support"
model_payment_type = "Payment Type"

using_proxy_to_connect = "Using Proxy to Connect"
using_proxy_to_connect_to = "Using Proxy to Connect to `{provider}`"

add_new_model = "New `{provider}` model: `{modified_model_alias}` added."
update_model = "`{provider}` `{model_alias}` Model Updated."
openai_supported_models_updated = "OpenAI supported models updated."

change_model_alias = "`{provider}` `{model_alias}` model alias changed to `{modified_model_alias}`."

openai_compatible_api_key = "OpenAI Compatible API Key"
openai_compatible_base_url = "OpenAI Compatible Base URL, ending with `v1`, such as: `https://api.openai.com/v1` for OpenaAI offical, `http://localhost:11434/v1` for Ollama."

delete_model_success = "`{provider}` `{model_alias}` model deleted successfully, page will reload."
delete_model_failed = "Failed to delete `{provider}` `{model_alias}` model."

ollama_server = "Ollama server"
ollama_server_running = "Running"
ollama_server_stopped = "Stopped"
ollama_server_status = "Ollama server status"
ollama_models_list = "Ollama models list"
select_a_model = "Select a Model"
ollama_model_brief_introduction = "Click to view a brief summary of the `{selected_model}` model"
ollama_model_detailed_introduction = "Click to view detailed information about the `{selected_model}` model"
ollama_model_alias_settings = "Ollama Model Alias Settings"
add_prefix_to_ollama_model_label = "Add a prefix to the Ollama model as the model alias"
ollama_model_alias_setting_option_1 = "Use `prefix-model` as the model alias"
ollama_model_alias_setting_option_2 = "Use `model` as the model alias"
ollama_model_alias_prefix_label = "Model Alias Prefix, default is `Ollama-`"
ollama_model_alias_prefix_option_updated = "Ollama model alias prefix updated.Page will reload shortly."
ollama_model_alias_example = "Example: `{prefix}{model}`"
add_prefix_to_ollama_model_updated = "Ollama model alias prefix updated. Page will reload shortly."
ollama_exclude_models_setting_title = "Ollama Exclude Models"
ollama_exclude_models_setting_description = "Set aliases for models to be excluded in Chat/Completion; these excluded models will not appear in the available model list for Chat/Completion. Typically, this refers to Ollama's embedding model, such as `nomic-embed-text`."
ollama_exclude_models_label = "Model aliases to be excluded, simply enter the model alias or the model name within the alias. Separate multiple models with a comma `,`:"
ollama_exclude_models_changed = "Ollama excluded models have been updated. The page will reload shortly to apply the changes."
ollama_exclude_models_no_changed = "No changes made to the excluded models."

backup_and_restore = "Backup and Restore"
backup_info = "Backup the configuration files of common settings and LLM settings (including prompts), and all {chat}/{completion} archives. \n - The backup file is saved in the `backups` folder within the `data` directory. \n - The backup file is named `{backup_file_name}`, e.g., `{current_file_name}`. "
restore_info = "Select a backup file to restore.\n - The restore operation will overwrite the current configuration file, model settings, and {chat}/{completion} archives. \n - The restore operation will reload the App to restore the backup configurations."
backup_error = "Backup failed."
restore_error = "Restore failed. Error occurred: {error}."
backup_success = "Backup successful, the backup file is saved as `{backup_file_fullpath}`."
restore_success = "Restore successful, the App will reload shortly."
select_backup_file = "Select a backup file to restore."
delete_backup_success = "Backup file `{backup_file}` deleted successfully. If this was an accidental deletion, please manually recover it from the `recycle bin`."
delete_backup_failed = "Failed to delete backup file `{backup_file}`."
no_backup_files = "No backup files found."
request_parameters_additional_info = "The following parameters are supported, with default values provided by the model provider. For more information, please refer to the model provider's official documentation."
no_available_provider = "No available provider found. Please enable a provider first."
# model parameter block
set_model_parameters = "Set `{model_alias}`/`{model}` Model Parameters: "
seed_description = "Set random seed for reproducibility, default is 1234."
temperature_description = "Set temperature value to control text generation creativity, higher value is more random."
top_p_description = "Set Top-p value to control text generation diversity, higher value is more diverse."
presence_penalty_description = "Set Presence Penalty value, higher value to enhance text generation diversity. Default is `0`."
frequency_penalty_description = "Set Frequency Penalty value, higher value to reduce model text repetition. Default is `0`."
repetition_penalty_description = "Set Repetition Penalty value, higher value to reduce model text repetition. Default is `1.0`, no repetition penalty."
repeat_penalty_description = "Set repetition penalty to control text generation quality, higher value is stricter. Default is `1.1`."
max_tokens_description = "Set the maximum text generation length. "
max_tokens_additional_info = "The maximum tokens for `{openai}` and `{qwen}` models are determined by the model."
max_output_tokens_description = "Set the maximum text generation length."
penalty_score_description = "Set penalty score to control text generation quality, higher value is stricter."
num_ctx_description = "Set context count to control text generation quality, automatically set by Ollama model."
top_k_description = "Set Top-k value to reduce the probability of generating meaningless content."
repeat_penalty_block = "Repetition Penalty"
num_predict_description = "Set prediction count to control text generation quality, set by Ollama model, -1 means no limit, -2 means fill context."
provider_models_only = "For `{provider}` models only."
disable_search_description = "Disable search, default is enabled."
enable_search_description = "Enable search, default is disabled. "
provider_default_value = "The default value of `{provider}` model is `{default}`"
request_parameters_description = "Request Parameters"
request_parameters_info = "Request parameters are used to control the quality and diversity of text generated by the model."
request_parameters_notice = "All request parameters are optional. Even without adding request parameters, the large model can still generate content correctly. However, using request parameters allows you to control the diversity of the generated content. If you use fixed request parameters, you may even be able to reproduce the content generated by the large model."
request_parameters_format = "Refer to the parameter descriptions in {llm_settings} for inputting request parameters. Use commas to separate multiple parameters. If left blank, no request parameters will be set."
unsupported_request_parameter = "Unsupported request parameter of `{parameter}`. Remove it and try again."
no_request_parameters_found = "No request parameters set for `{model_alias}`."
recommended_request_parameters_title = "Common request parameters from the official API document"
recommended_request_parameters_description = "Below are the common request parameters provided by the official API documentation. Please note that even for the same service provider, different models may support different request parameters."
recommended_request_parameters_by_provider = "**{provider}**: `{parameters}`."
official_api_document = "Official API Document for {provider}: {url} ."
openai_compatible_request_parameters_description = "`may` or `may not` support request parameters. For details, please refer to the official documentation of the model being added."
ollama_request_parameters_updated = "Ollama model request parameters updated."
updating_ollama_request_parameters = "Updating Ollama model request parameters..."
ollama_model_request_parameters_updated = "Ollama Model `{model}` request parameters updated."
ollama_request_parameters_not_set = "No request parameters set for Ollama model."

# request parameters
customize_request_parameters = "Customize Request Parameters"
customize_request_parameters_description = "You can add, modify, or delete request parameters here. Once added, they can be used in the model settings. Currently, the supported request parameter types are `{param_types}`. Please **note** when adding custom parameters: \n - The request parameter name must not duplicate predefined parameters. \n - The request parameter name must be supported by the model. \n - Adding the string `stop` is prohibited to avoid improper reception of model responses."
no_customized_request_parameters = "No customized request parameters found. Add a new request parameter first."
request_parameter_name = "Request Parameter Name"
request_parameter_name_empty = "Request parameter name cannot be empty."
request_parameter_name_duplicated = "Request parameter name `{parameter}` duplicated."
request_parameter_type = "Request parameter type"
request_parameter_default_value = 'Default Value'
request_parameter_min_value = "Minimum Value"
request_parameter_max_value = "Maximum Value"
request_parameter_adjust_step = "Adjustment Step"
request_parameter_description = "Request parameter description"
request_parameter_description_notice = "The description of the request parameter will be added to all existing language files."
add_request_parameter = "Add Request Parameter"
add_request_parameter_success = "Request parameter `{parameter}` added successfully."
add_request_parameter_description = "The description of request parameter `{parameter}` added to language files `{files}`."
add_variable_to_language_files_error = "Failed to add request parameter `{parameter}` to language files."
select_a_request_parameter = "Select a Request Parameter"
update_request_parameter = "Update Request Parameter"
update_request_parameter_success = "Request parameter `{parameter}` updated successfully."
delete_request_parameter = "Delete Request Parameter"
request_parameter_name_to_delete = "Request parameter `{parameter}` to be deleted. "
delete_request_parameter_success = "Request parameter `{parameter}` deleted successfully. Page will reload shortly."
delete_request_parameter_description = "Request parameter `{parameter}` deleted from language files `{files}`."
request_parameter_not_found_in_language_files = "Request parameter `{parameter}` not found in language files."
default_value_out_of_range = "Default value out of range, should be between `{min_value}` and `{max_value}`."
min_value_greater_than_max_value = "Minimum value should be less than maximum value."
empty_variable_value = "{variable} cannot be empty."
querying_models = "Querying models of `{provider}`..."
archive_file_naming_rule = "archive file naming rule"
archive_file_naming_rule_description = "The naming rule for the archive file (`*.json`), currently supports only three predefined variables: `{{timestamp}}`, `{{type}}`, and `{{model}}`. \n - `{{timestamp}}` represents the current time, such as `{timestamp}`. \n - `{{type}}` is either `chat` or `completion`. \n - `{{model}}` is the model name. \n\n The default naming format is `[{{timestamp}}]-[{{type}}]-[{{model}}]`. For example, `{example}`."
archive_file_naming_rule_updated = "Archive file naming rule updated."
archive_file_naming_rule_invalid = "Invalid archive file naming rule. Currently supports only three predefined variables: `{timestamp}`, `{type}`, and `{model}`."
add_common_request_parameters_to_new_model = "Add Common Request Parameters to New Model"
add_common_request_parameters_when_adding_model_label = "Add common request parameters when adding a model."
add_common_request_parameters_when_adding_model_description = "Add common request parameters to the model when adding a new model. The common request parameters are predefined as shown in `{section}`."
add_common_request_parameters_to_new_model_option_updated = "The option to add common request parameters to a new model has been updated."
# customized request parameters
customized_request_param_new_boolean_param_description = "customized boolean parameter."
customized_request_param_new_float_param_description = "customized float parameter."

